{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c3e393-3179-496f-a1c8-bc3479df5bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.29.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zee jay\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdaf0b3d-be07-41e5-9d29-dff84fc70da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['cropped_image', 'labels', 'labelNames'],\n",
      "    num_rows: 13434\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nguyenkhoa/celeba-spoof-for-face-antispoofing-test\")\n",
    "train_ds = ds[\"test\"].train_test_split(train_size=0.2, seed=42)[\"train\"]\n",
    "print(train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e37cd53-a24e-4aa3-8f1d-80e8bcf52e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping image 443 because it's None\n",
      "Skipping image 468 because it's None\n",
      "Skipping image 855 because it's None\n",
      "Skipping image 1134 because it's None\n",
      "Skipping image 1317 because it's None\n",
      "Skipping image 1521 because it's None\n",
      "Skipping image 1740 because it's None\n",
      "Skipping image 1994 because it's None\n",
      "Skipping image 2051 because it's None\n",
      "Skipping image 2156 because it's None\n",
      "Skipping image 2313 because it's None\n",
      "Skipping image 2550 because it's None\n",
      "Skipping image 2662 because it's None\n",
      "Skipping image 2804 because it's None\n",
      "Skipping image 3009 because it's None\n",
      "Skipping image 3112 because it's None\n",
      "Skipping image 3249 because it's None\n",
      "Skipping image 3264 because it's None\n",
      "Skipping image 3463 because it's None\n",
      "Skipping image 3526 because it's None\n",
      "Skipping image 3591 because it's None\n",
      "Skipping image 3956 because it's None\n",
      "Skipping image 4052 because it's None\n",
      "Skipping image 4059 because it's None\n",
      "Skipping image 4095 because it's None\n",
      "Skipping image 4437 because it's None\n",
      "Skipping image 4553 because it's None\n",
      "Skipping image 4824 because it's None\n",
      "Skipping image 4887 because it's None\n",
      "Skipping image 4970 because it's None\n",
      "Skipping image 5074 because it's None\n",
      "Skipping image 5403 because it's None\n",
      "Skipping image 5582 because it's None\n",
      "Skipping image 5635 because it's None\n",
      "Skipping image 5683 because it's None\n",
      "Skipping image 5792 because it's None\n",
      "Skipping image 5796 because it's None\n",
      "Skipping image 5906 because it's None\n",
      "Skipping image 5914 because it's None\n",
      "Skipping image 5963 because it's None\n",
      "Skipping image 6073 because it's None\n",
      "Skipping image 6248 because it's None\n",
      "Skipping image 6353 because it's None\n",
      "Skipping image 6381 because it's None\n",
      "Skipping image 6657 because it's None\n",
      "Skipping image 6705 because it's None\n",
      "Skipping image 7094 because it's None\n",
      "Skipping image 7109 because it's None\n",
      "Skipping image 7121 because it's None\n",
      "Skipping image 7234 because it's None\n",
      "Skipping image 7259 because it's None\n",
      "Skipping image 7370 because it's None\n",
      "Skipping image 7388 because it's None\n",
      "Skipping image 7391 because it's None\n",
      "Skipping image 7713 because it's None\n",
      "Skipping image 8282 because it's None\n",
      "Skipping image 8317 because it's None\n",
      "Skipping image 8380 because it's None\n",
      "Skipping image 8651 because it's None\n",
      "Skipping image 8746 because it's None\n",
      "Skipping image 8845 because it's None\n",
      "Skipping image 8890 because it's None\n",
      "Skipping image 8892 because it's None\n",
      "Skipping image 8903 because it's None\n",
      "Skipping image 8997 because it's None\n",
      "Skipping image 9251 because it's None\n",
      "Skipping image 9550 because it's None\n",
      "Skipping image 9878 because it's None\n",
      "Skipping image 9879 because it's None\n",
      "Skipping image 10030 because it's None\n",
      "Skipping image 10106 because it's None\n",
      "Skipping image 10168 because it's None\n",
      "Skipping image 10278 because it's None\n",
      "Skipping image 10543 because it's None\n",
      "Skipping image 10553 because it's None\n",
      "Skipping image 10673 because it's None\n",
      "Skipping image 10776 because it's None\n",
      "Skipping image 10890 because it's None\n",
      "Skipping image 11041 because it's None\n",
      "Skipping image 11302 because it's None\n",
      "Skipping image 11308 because it's None\n",
      "Skipping image 11356 because it's None\n",
      "Skipping image 11413 because it's None\n",
      "Skipping image 11632 because it's None\n",
      "Skipping image 12086 because it's None\n",
      "Skipping image 12131 because it's None\n",
      "Skipping image 12140 because it's None\n",
      "Skipping image 12201 because it's None\n",
      "Skipping image 12295 because it's None\n",
      "Skipping image 12524 because it's None\n",
      "Skipping image 12651 because it's None\n",
      "Skipping image 13084 because it's None\n",
      "20% training data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "train_dir = \"celeba_spoof/train\"\n",
    "\n",
    "os.makedirs(os.path.join(train_dir, \"real\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"spoof\"), exist_ok=True)\n",
    "\n",
    "for i, example in enumerate(train_ds):\n",
    "    img = example[\"cropped_image\"]\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Skipping image {i} because it's None\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(img, Image.Image):\n",
    "        img = Image.open(img)\n",
    "\n",
    "    label = \"real\" if example[\"labels\"] == 0 else \"spoof\"\n",
    "    \n",
    "    img_path = os.path.join(train_dir, label, f\"{i}.jpg\")\n",
    "    img.save(img_path)\n",
    "\n",
    "print(\"20% training data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0938d9-7089-41a8-b643-263c93cf13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "model_name = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02dd8216-c0e1-4d1f-afff-7a936955e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTImageProcessor {\n",
      "  \"do_convert_rgb\": null,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d07901-9dc0-4c4a-bf81-bfdd811405c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    if example[\"cropped_image\"] is None:\n",
    "        return None\n",
    "\n",
    "    inputs = processor(example[\"cropped_image\"], return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = torch.tensor(example[\"labels\"])\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8097d9f2-1d72-4658-b4b7-71fd6193176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "processed_ds = train_ds.map(process_example, remove_columns=[\"cropped_image\", \"labelNames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea1915b-671f-4587-aded-27ca89a77475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'pixel_values'],\n",
      "    num_rows: 13342\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(processed_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d180a98-6dc9-4381-9a6a-764601b637fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([torch.tensor(x['pixel_values']) for x in batch])\n",
    "    labels = torch.tensor([x['labels'] for x in batch])\n",
    "    \n",
    "    pixel_values = pixel_values.squeeze(1)\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d54b7283-c878-43c2-b9d0-335e2a437485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)  # Convert logits to class labels\n",
    "    return metric.compute(predictions=predictions, references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9dd2a3a-1563-48f0-b116-4d1bf3e92a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"real\", \"spoof\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2471180e-aa15-4cb9-b619-cae4c5beefe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"real\",\n",
      "    \"1\": \"spoof\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"real\": \"0\",\n",
      "    \"spoof\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): label for i, label in enumerate(labels)},\n",
    "    label2id={label: str(i) for i, label in enumerate(labels)}\n",
    ")\n",
    "\n",
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9583832b-5a46-4ab9-8f3a-2187523cf5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zee Jay\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-celeba-spoof\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a5c09c8-2bd7-445b-8f71-47f0f718d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "splits = processed_ds.train_test_split(test_size=0.2, seed=42)\n",
    "prepared_ds = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"validation\": splits[\"test\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ac645a0-8ad3-4ae5-a584-8b2d12deccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zee Jay\\AppData\\Local\\Temp\\ipykernel_13812\\4284488981.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=processor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9183670-291d-4951-81d2-da4b117b3de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2672' max='2672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2672/2672 59:32, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.996628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.997377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.018983</td>\n",
       "      <td>0.996628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.998876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =          4.0\n",
      "  total_flos               = 3081083316GF\n",
      "  train_loss               =       0.0218\n",
      "  train_runtime            =   0:59:35.38\n",
      "  train_samples_per_second =       11.941\n",
      "  train_steps_per_second   =        0.747\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48fb2e6b-73f1-4dd4-bd0b-732ed6362a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9989\n",
      "Validation Precision: 0.9989\n",
      "Validation Recall: 0.9989\n",
      "Validation F1-score: 0.9989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = trainer.predict(prepared_ds[\"validation\"])\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)  \n",
    "labels = predictions.label_ids  \n",
    "\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision = precision_score(labels, preds, average='weighted')\n",
    "recall = recall_score(labels, preds, average='weighted')\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {precision:.4f}\")\n",
    "print(f\"Validation Recall: {recall:.4f}\")\n",
    "print(f\"Validation F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e77e30-7c6b-4bb8-8b4c-ab4709222e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
